---
title: 'Exploring VQ-VAEs'
date: 2023-09-17
tags: ['ECG', 'Autoencoder', 'VQ-VAE']
author: Samuel
---


## Introduction

VQ-VAEs[^VQ-VAE] are a type of generative model that merge the strengths of traditional autoencoders and vector quantization, providing a more robust and efficient method for data compression and generation. [More on VQ-VAEs](#vq-vaes-a-brief-overview)

In this blog post, we'll delve into my recent research efforts centered on using VQ-VAEs for encoding 1D dataâ€”specifically, Electrocardiogram (ECG) readings.

## The Necessity of ECG Data Encoding
{% algrtmImgBanner VQ-VAEs/pic3.jpg peak %}
#### The Critical Role of ECG in Healthcare
Electrocardiogram (ECG) readings play a pivotal role in modern healthcare, providing valuable insights into a person's heart condition. These readings can identify irregular heart rhythms, signs of coronary artery disease, and even the effects of drugs on the heart. However, the high-resolution, multichannel 1D data streams generated by ECG machines present significant challenges in terms of storage and analysis.
#### The Challenge of Multichannel 1D Signals
Traditional ECG systems often record data from multiple leads, resulting in multichannel 1D signals. Each channel provides a different perspective on heart activity, making the overall dataset rich but also dense and hard to compress. Efficiently encoding this multichannel 1D data is crucial for both real-time monitoring and long-term storage, particularly when remote monitoring or telemedicine is involved.

By focusing on ECG data, we aim to develop a compression and encoding technique that is both efficient and retains critical clinical information, a task for which VQ-VAEs show particular promise.


## VQ-VAEs: A Brief Overview
{% algrtmImgBanner VQ-VAEs/pic1.jpg shadow %}

#### Differences from Variational Autoencoders (VAEs)
Variational Autoencoders (VAEs)[^VAE] are another popular choice for learning generative models. VAEs consist of an encoder, a decoder, and an additional probabilistic layer that models the latent space as a distribution. While powerful, VAEs often suffer from a problem known as "posterior collapse",[^posterior] where the latent variables become almost useless for generating the data because the variational posterior is too close to the prior.

#### How VQ-VAEs Address This Issue
VQ-VAEs introduce an additional layer of complexity with vector quantization, which addresses this issue. In VQ-VAEs, the encoder's output is not used directly for decoding. Instead, it is mapped to the nearest vector in a predefined codebook. This discrete representation, sourced from the codebook, is then used by the decoder for reconstruction.

What this achieves is a more balanced relationship between the encoder and the decoder:

- **Richer Latent Space:** Because the encoder's output is quantized to specific codebook vectors, it is forced to create a more informative latent representation to minimize the quantization error.

- **More Controlled Reconstructions:** The use of a codebook makes the latent space discrete, allowing for more consistent and controlled reconstructions, as each code vector has a specific meaning or representation.

- **Reduced Decoder "Cleverness":** The codebook acts as a form of "regularization," preventing the decoder from becoming too adept at reconstructing from inadequate encodings, thereby encouraging the encoder to be more descriptive.

#### Toy Example
```python
# necessary imports
import torch
import torch.nn as nn
import torch.optim as optim

# Define the encoder
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.layer = nn.Linear(10, 5)
        
    def forward(self, x):
        return self.layer(x)

# Define the codebook
class Codebook(nn.Module):
    def __init__(self, num_codes, code_dim):
        super(Codebook, self).__init__()
        self.codebook = nn.Parameter(torch.randn(num_codes, code_dim))
        
    def forward(self, z):
        distances = ((z.unsqueeze(1) - self.codebook.detach().unsqueeze(0))**2).sum(-1)
        indices = torch.argmin(distances, dim=1)
        return self.codebook[indices]

# Define the decoder
class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.layer = nn.Linear(5, 10)
        
    def forward(self, z):
        return self.layer(z)

# Initialize the model
encoder = Encoder()
codebook = Codebook(num_codes=64, code_dim=5)
decoder = Decoder()

optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)

# Training loop
for epoch in range(1000):
    # Simulated input data
    x = torch.randn(32, 10)
    
    # Forward pass
    z_e = encoder(x)
    z_q = codebook(z_e)
    x_recon = decoder(z_q)
    
    # Reconstruction loss
    loss = ((x - x_recon)**2).mean()
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 100 == 0:
        print(f"Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}")
```


## Diving into the Results
{% algrtmImgBanner VQ-VAEs/pic4.jpg headphones %}

Having explored the nuances of VQ-VAEs, it's time to delve into their practical applications. We applied the VQ-VAE model to ECG data, aiming to explore how well it perform in real-world scenarios. Let's jump straight into the outcomes.

Our experimentation with ECG data yielded promising results in terms of both compression and reconstruction quality. The model was able to capture critical features like R-peaks and QRS complexes, essential for cardiac diagnosis.

{% algrtmImg VQ-VAEs/ecg-vqvae-reconstruction.png ecg 300px %}


[^VQ-VAE]: [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
[^VAE]: [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)
[^posterior]: [Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders](https://arxiv.org/abs/2306.05023)
